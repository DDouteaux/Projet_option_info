{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf, numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the data in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('simple2_save.pickle', 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    adj_pos = save['adj_pos'][:100]\n",
    "    adj_neg = save['adj_neg'][:100]\n",
    "    adverb_no_change = save['adverb_no_change']\n",
    "    adverb_change = save['adverb_change']\n",
    "    adverb_neg = save['adverb_neg']\n",
    "    adverb_pos = save['adverb_pos'] \n",
    "    dico_embedding = save['dico_embedding'] \n",
    "adjectives = [adj_neg, adj_pos]\n",
    "adverbs = [adverb_no_change, adverb_change, adverb_neg, adverb_pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data parsing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos contient 2041 mots\n",
      "pos contient 2006 mots\n"
     ]
    }
   ],
   "source": [
    "all_positive = 'positive-words.txt'\n",
    "with open(all_positive, 'r') as f:\n",
    "    all_positive = f.read().splitlines()\n",
    "print('pos contient %d mots' % len(all_positive))\n",
    "all_positive = [p for p in all_positive if len(p) and not p.startswith(';')]\n",
    "print('pos contient %d mots' % len(all_positive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restriction to the strict adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos contient 365 mots\n"
     ]
    }
   ],
   "source": [
    "pos = [p for p in all_positive if nltk.pos_tag([p])[0][1] in ['JJ']] # Pos for Part Of Speech\n",
    "print('pos contient %d mots' % len(pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same with negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg contient 3999 mots\n",
      "neg contient 3927 mots\n"
     ]
    }
   ],
   "source": [
    "all_negative = 'negative-words.txt'\n",
    "with open(all_negative, 'r') as f:\n",
    "    all_negative = []\n",
    "    for _ in range(4000):\n",
    "        try:\n",
    "            all_negative.append(f.readline())\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "        \n",
    "print('neg contient %d mots' % len(all_negative))\n",
    "all_negative = [n.strip('\\n') for n in all_negative if not n.startswith(';')]\n",
    "all_negative = [n for n in all_negative if len(n)]\n",
    "print('neg contient %d mots' % len(all_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg contient 460 mots\n"
     ]
    }
   ],
   "source": [
    "neg = [n for n in all_negative if nltk.pos_tag([n])[0][1] == 'JJ' and \n",
    "                          not n.endswith('ly') and '-' not in n]\n",
    "print('neg contient %d mots' % len(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adjectives = [neg, pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout de mots qui meublent, qui changent le sens de la phrase ou non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adverb_change = ['never', 'not', 'rarely', 'hardly', 'barely']\n",
    "adverb_no_change = ['always', 'sometimes', 'just', 'often', 'usually']\n",
    "adverb_neg = ['rudely', 'unfortunately', 'arrogantly', 'cruelly', 'deceitfully']\n",
    "adverb_pos = ['beautifully', 'awesomely', 'happily', 'brightly', 'elegantly']\n",
    "adverbs = [adverb_no_change, adverb_change, adverb_neg, adverb_pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather all the words, get them in a dense representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_used = set(['this', 'is'] + neg + pos + adverb_neg + adverb_pos + adverb_change\n",
    "                 + adverb_no_change)\n",
    "dico_embedding = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'len(dico_embedding) = 804'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'len(words_used) = 804'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'The words set() are not in the corpus'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../glove.6B.50d.txt', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        word, *vector = line.split()\n",
    "        if word in words_used:\n",
    "            dico_embedding[word] = vector\n",
    "    'len(dico_embedding) = %d' % len(dico_embedding)\n",
    "    'len(words_used) = %d' % len(words_used)\n",
    "    'The words %s are not in the corpus' % (words_used - set(dico_embedding.keys()))\n",
    "\n",
    "to_get_rid_of = words_used - set(dico_embedding.keys())\n",
    "dico_embedding['This'] = dico_embedding['this']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longueur de pos AVANT élimination des mots rares : 343\n",
      "Longueur de pos APRES : 343\n",
      "Longueur de neg AVANT élimination des mots rares : 440\n",
      "Longueur de neg APRES : 440\n",
      "Longueur de adverb_pos AVANT élimination des mots rares : 5\n",
      "Longueur de adverb_pos APRES : 5\n",
      "Longueur de adverb_neg AVANT élimination des mots rares : 5\n",
      "Longueur de adverb_neg APRES : 5\n",
      "Longueur de adverb_change AVANT élimination des mots rares : 5\n",
      "Longueur de adverb_change APRES : 5\n",
      "Longueur de adverb_no_change AVANT élimination des mots rares : 5\n",
      "Longueur de adverb_no_change APRES : 5\n"
     ]
    }
   ],
   "source": [
    "data_names = ['pos', 'neg', 'adverb_pos', 'adverb_neg', 'adverb_change', 'adverb_no_change']\n",
    "for dn in data_names:\n",
    "    data = eval(dn)\n",
    "    print('Longueur de %s AVANT élimination des mots rares : %d' % (dn, len(data)))\n",
    "    data = list(set(data) - to_get_rid_of)\n",
    "    print('Longueur de %s APRES : %d' % (dn, len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('simple2_save.pickle', 'wb') as f:\n",
    "    pickle.dump({'adj_pos': pos, \n",
    "                 'adj_neg': neg,\n",
    "                 'adverb_no_change': adverb_no_change, \n",
    "                 'adverb_change': adverb_change,\n",
    "                 'adverb_neg': adverb_neg,\n",
    "                 'adverb_pos': adverb_pos, \n",
    "                 'dico_embedding': dico_embedding}, \n",
    "                f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the data in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('simple2_save.pickle', 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    adj_pos = save['adj_pos']\n",
    "    adj_neg = save['adj_neg']\n",
    "    adverb_no_change = save['adverb_no_change']\n",
    "    adverb_change = save['adverb_change']\n",
    "    adverb_neg = save['adverb_neg']\n",
    "    adverb_pos = save['adverb_pos'] \n",
    "    dico_embedding = save['dico_embedding'] \n",
    "adjectives = [adj_neg, adj_pos]\n",
    "adverbs = [adverb_no_change, adverb_change, adverb_neg, adverb_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longueur de adverb_no_change : 5\n",
      "Longueur de dico_embedding : 805\n",
      "Longueur de adj_pos : 100\n",
      "Longueur de adverb_neg : 5\n",
      "Longueur de adj_neg : 100\n",
      "Longueur de adverb_pos : 5\n",
      "Longueur de adverb_change : 5\n"
     ]
    }
   ],
   "source": [
    "for data_name in save.keys():\n",
    "    print(\"Longueur de %s : %d\" % (data_name, len(eval(data_name))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routines pour générer les données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Example_Generator(object):\n",
    "    \n",
    "    def __init__(self, adjectives, adverbs):\n",
    "        self.adverbs = adverbs\n",
    "        n = min(len(adjs) for adjs in adjectives)\n",
    "        permut = random.sample(range(n), 5*n//6)\n",
    "        self.training_adjectives = [[adjs[i] for i in permut] for adjs in adjectives]\n",
    "        self.validation_adjectives = [[adjs[i] for i in range(n) if i not in permut] \n",
    "                                 for adjs in adjectives]\n",
    "        self.adjectives = adjectives        \n",
    "        \n",
    "    def generate_example(self):\n",
    "        adv_ind = random.randint(0,3)\n",
    "        adv = random.choice(self.adverbs[adv_ind])\n",
    "        if adv_ind in [0, 2]: # on peut choisir parmi tous les adjectifs\n",
    "            adj_ind = random.randint(0,1)\n",
    "            adj = random.choice(self.adjectives[adj_ind])\n",
    "        else: # adv_ind in [1, 3], on ne choisit que parmi les adjectifs de training\n",
    "            adj_ind = random.randint(0,1)\n",
    "            adj = random.choice(self.training_adjectives[adj_ind])\n",
    "         \n",
    "        sentence = ['This', 'is', adv, adj]\n",
    "        label = int(adv_ind == 0 and adj_ind or # on NE change PAS le sentiment de l'adjectif\n",
    "                    adv_ind == 1 and not adj_ind or # on change le sentiment de l'adjectif\n",
    "                    adv_ind == 2 and 0 or # l'adverbe impose le sentiment négatif\n",
    "                    adv_ind == 3) # l'adverbe impose le sentiment positif \n",
    "        return sentence, label\n",
    "    \n",
    "    def generate_validation(self):\n",
    "        for i, adjs in enumerate(self.validation_adjectives):\n",
    "            for adj in adjs:\n",
    "                for adv in self.adverbs[1]:\n",
    "                    label = int(not i)\n",
    "                    yield ['This', 'is', adv, adj], label\n",
    "                for adv in self.adverbs[3]:\n",
    "                    label = 1\n",
    "                    yield ['This', 'is', adv, adj], label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_batch(example_generator, batch_size, size_embedding=50):\n",
    "    x = np.empty([4, batch_size, size_embedding])\n",
    "    y = np.empty([1, batch_size, 1])\n",
    "    \n",
    "    for j in range(batch_size):\n",
    "        sentence_split, label = example_generator.generate_example()\n",
    "        \n",
    "        for i, w in enumerate(sentence_split):\n",
    "            x[i,j,:] = dico_embedding[w]\n",
    "            \n",
    "        y[0,j,0] = label\n",
    "    return x, y\n",
    "\n",
    "def generate_validation_set(example_generator, size_embedding=50, validation_size=None):\n",
    "    max_size = (2*len(example_generator.validation_adjectives[0])*\n",
    "                2*len(adverbs[0]))\n",
    "    if not validation_size or validation_size > max_size: \n",
    "        validation_adjectives = max_size\n",
    "    x = np.empty([4, validation_size, size_embedding])\n",
    "    y = np.empty([1, validation_size, 1])\n",
    "    \n",
    "    gen = example_generator.generate_validation()\n",
    "    for j in range(validation_size):\n",
    "        sentence_split, label = next(gen)\n",
    "        \n",
    "        for i, w in enumerate(sentence_split):\n",
    "            x[i,j,:] = dico_embedding[w]\n",
    "            \n",
    "        y[0,j,0] = label\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph building and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = 50\n",
    "OUTPUT_SIZE = 1\n",
    "RNN_HIDDEN = 100\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "DROPOUT = 0\n",
    "\n",
    "TINY = 1e-7\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 500\n",
    "ITERATONS_PER_EPOCH = 20\n",
    "BATCH_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#############             Graph Definition             ##############\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "    # Definition of the inputs and outputs\n",
    "    inputs = tf.placeholder(tf.float32, (None, None, INPUT_SIZE))\n",
    "    labels = tf.placeholder(tf.float32, (None, None, OUTPUT_SIZE))\n",
    "    \n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(RNN_HIDDEN)\n",
    "    \n",
    "    # maybe add dropout\n",
    "    if DROPOUT:\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=0.5, output_keep_prob=0.8)\n",
    "    \n",
    "    # Definition of the initial state\n",
    "    batch_size = tf.shape(inputs)[1]\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Computation of the outputs and states\n",
    "    with tf.variable_scope('lstm_weights'):\n",
    "        rnn_outputs, rnn_states = tf.nn.dynamic_rnn(cell, inputs, \n",
    "                                                    initial_state=initial_state, \n",
    "                                                    time_major=True)\n",
    "        ## First by taking only the last step\n",
    "        final_outputs = tf.slice(rnn_outputs, begin=(3,0,0), size=(1,-1,RNN_HIDDEN))\n",
    "        ## Then by averaging on all states\n",
    "        final_outputs = tf.reduce_mean(rnn_outputs, axis=0, keep_dims=True)\n",
    "        _ = tf.summary.histogram('hidden_state', rnn_states)\n",
    "        \n",
    "\n",
    "    # Projection of the outputs\n",
    "    final_projection = lambda x: tf.contrib.layers.linear(x, num_outputs=OUTPUT_SIZE, \n",
    "                                                          activation_fn=tf.nn.sigmoid)\n",
    "    # Application of final projection to the outputs\n",
    "    logits = tf.map_fn(final_projection, final_outputs)\n",
    "\n",
    "    # Loss\n",
    "    loss = -(labels*tf.log(logits + TINY) + (1.0 - labels)*tf.log(1.0 - logits + TINY))\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    # train_optimizer\n",
    "    train_optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    # For validation purpose\n",
    "    accuracy = tf.reduce_mean(tf.cast(abs(logits - labels) < 0.5, tf.float32))\n",
    "    \n",
    "    # Summaries\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter('/home/louis/python/notebooks/.tensorflow_logs_dir/', \n",
    "                                   graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0, Epoch Loss = 0.02386198\n",
      "Accuracy = 64.0\n",
      "Iteration : 1, Epoch Loss = 0.01795882\n",
      "Accuracy = 63.7\n",
      "Iteration : 2, Epoch Loss = 0.01659119\n",
      "Accuracy = 76.0\n",
      "Iteration : 3, Epoch Loss = 0.01122537\n",
      "Accuracy = 84.7\n",
      "Iteration : 4, Epoch Loss = 0.00750723\n",
      "Accuracy = 95.0\n",
      "Iteration : 5, Epoch Loss = 0.00611988\n",
      "Accuracy = 95.0\n",
      "Iteration : 6, Epoch Loss = 0.00372319\n",
      "Accuracy = 96.7\n",
      "Iteration : 7, Epoch Loss = 0.00362951\n",
      "Accuracy = 96.7\n",
      "Iteration : 8, Epoch Loss = 0.00270680\n",
      "Accuracy = 96.7\n",
      "Iteration : 9, Epoch Loss = 0.00281181\n",
      "Accuracy = 96.7\n",
      "Iteration : 10, Epoch Loss = 0.00196158\n",
      "Accuracy = 96.7\n",
      "Iteration : 11, Epoch Loss = 0.00155939\n",
      "Accuracy = 96.7\n",
      "Iteration : 12, Epoch Loss = 0.00118558\n",
      "Accuracy = 97.0\n",
      "Iteration : 13, Epoch Loss = 0.00157578\n",
      "Accuracy = 96.7\n",
      "Iteration : 14, Epoch Loss = 0.00076268\n",
      "Accuracy = 96.7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5b9cc7eea917>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             epoch_loss, _, summaries = session.run([loss, train_optimizer, merged], \n\u001b[1;32m     15\u001b[0m                                                    feed_dict={inputs:x, \n\u001b[0;32m---> 16\u001b[0;31m                                                               labels:y})\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Summaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mITERATONS_PER_EPOCH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/louis/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "########                         Training Loop                     ########\n",
    "###########################################################################\n",
    "\n",
    "eg = Example_Generator(adjectives, adverbs)\n",
    "valid_x, valid_y = generate_validation_set(eg, validation_size=300)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        for j in range(ITERATONS_PER_EPOCH):\n",
    "            x, y = generate_batch(eg, batch_size=BATCH_SIZE)\n",
    "            epoch_loss, _, summaries = session.run([loss, train_optimizer, merged], \n",
    "                                                   feed_dict={inputs:x, \n",
    "                                                              labels:y})\n",
    "            # Summaries\n",
    "            ind = i*ITERATONS_PER_EPOCH + j\n",
    "            if ind%10 == 0:\n",
    "                writer.add_summary(summaries, ind)\n",
    "        epoch_loss /= ITERATONS_PER_EPOCH\n",
    "        valid_accuracy = session.run(accuracy, \n",
    "                                     feed_dict={inputs:valid_x, labels:valid_y})\n",
    "        print('Iteration : %d, Epoch Loss = %.8f' % (i, epoch_loss))\n",
    "        print('Accuracy = %.1f' % (valid_accuracy*100.))\n",
    "        if valid_accuracy == 1 or i == 30: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "graph = tf.Graph()\n",
    "graph.as_default()\n",
    "isess = tf.InteractiveSession(graph=graph)\n",
    "\n",
    "batch_size = 98\n",
    "eg = Example_Generator(adjectives, adverbs)\n",
    "valid_x, valid_y = generate_batch(eg, batch_size)\n",
    "inputs, labels = tf.constant(valid_x), tf.constant(valid_y)\n",
    "\n",
    "INPUT_SIZE = 50\n",
    "OUTPUT_SIZE = 1\n",
    "RNN_HIDDEN = 200\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "TINY = 1e-7\n",
    "# Definition of the cell\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=RNN_HIDDEN, state_is_tuple=True)\n",
    "\n",
    "# Definition of the initial state\n",
    "initial_state = cell.zero_state(batch_size, dtype=np.float64)\n",
    "\n",
    "# Computation of the outputs and states\n",
    "rnn_outputs, rnn_states = tf.nn.dynamic_rnn(cell, inputs, \n",
    "                                            initial_state=initial_state, time_major=True)\n",
    "final_output = tf.slice(rnn_outputs, begin=(3,0,0), size=(1,-1,-1))\n",
    "final_output = tf.reduce_mean(rnn_outputs, axis=0, keep_dims=True)\n",
    "\n",
    "# Definition of the outputs\n",
    "final_projection = lambda x: tf.contrib.layers.linear(x, num_outputs=OUTPUT_SIZE, \n",
    "                                                      activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "# Application of final projection to the outputs\n",
    "logits = tf.map_fn(final_projection, final_output)\n",
    "\n",
    "# Loss\n",
    "loss = -(labels*tf.log(logits + TINY) + (1.0 - labels)*tf.log(1.0 - logits + TINY))\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# Train optimizer\n",
    "train_optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(loss)\n",
    "\n",
    "# Boring stuff\n",
    "isess.run(tf.global_variables_initializer());"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ro, fo, rs, l, loss = isess.run([rnn_outputs, final_output, rnn_states, logits, loss])\n",
    "#to = isess.run(train_optimizer)\n",
    "#ts, l = isess.run([train_optimizer, loss])\n",
    "ts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ro.shape, fo.shape, rs.c.shape, rs.h.shape, l.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "isess.run(labels).shape"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
